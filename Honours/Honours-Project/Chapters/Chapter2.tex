% Chapter 2

\chapter{Methodology} \label{c2} % Main chapter title

%\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Survey Design} \label{c2.1}
When analysing survey data sets in the {\bf survey} package, a survey design ({\bf svydesign()}) is always required. The survey design object combines the data set and all the survey design information needed to analyse it. These objects are used by the survey modelling and summary functions.\\

The set of functions that {\bf surveydb} provides also adapted this concept but with a few modifications, it uses the {\bf R6} \citep{R6package} class system which is encapsulated object orientation programming and is different to the standard {\bf S3} and {\bf S4} in base {\sf R} which uses functional object orientation programming. The main difference between the two is that {\bf S3} and {\bf S4} methods and objects are separate and in {\bf R6}, object contains methods and data. \\

The advantage of encapsulated object orientation programming is that information within the objects are not computed unless it is needed, for example when the sum of all sampling weights are needed, we can compute it by using a method within the object and the value also updates when it's been called on a subset of the object.

\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
    id = SDMVPSU, data = nhdb)
> nh.dbsurv$getwt()
[1] 306590681

> nh.dbsurv$subset(Race3 == 3)$getwt()
[1] 192721267
\end{lstlisting}

Therefore, it is much more time efficient than creating a survey design that contains all the information, since the user may not need every information within the object. \\

% Another advantage of this approach is that there will not be any extra generic methods that can be called directly by the user, because these methods are designed specifically for a type of object and usually it should not be called directly by the user.


The {\ttfamily svydbdesign()} function, has four basic arguments,
\begin{center}
    {\ttfamily svydbdesign(st = NULL, id = NULL, wt, data)}
\end{center}

\begin{itemize}
\item $st$ = Column name specifying the strata column. $NULL$ for no strata. 

\item $id$ = Column name specifying the cluster column. $NULL$ for no cluster. 

\item $wt$ = Column name specifying the sampling weights column.

\item $data$ = A data frame or sql table of the survey data set.
\end{itemize}

\\

When the {\ttfamily svydb.design} object is called directly, a brief description of the design will be displayed.

\begin{lstlisting}
> nh.dbsurv
svydb.design, 9756 observation(s), 31 Clusters
\end{lstlisting}

\\

Functions within the {\bf svydb.design} object for users to use are,

\begin{lstlisting}
Classes 'svydb.design', 'R6' <svydb.design>
.
.
clone: function (deep = FALSE) 
getmh: function () 
getwt: function () 
subset: function (..., logical = T) 
subset_rows: function (from, to) 
\end{lstlisting}


\begin{itemize}
\item $clone()$ = Create a clone of the object.

\item $getmh()$ = A table indicating strata and cluster information.

\item $getwt()$ = Compute the sum of the sampling weights.

\item $subset()$ = Subset the design by conditions, similar to $base::subset$. \\
                    e.g.({\ttfamilyd design\$subset(Race3 == 3)} 

\item $subset\_rows()$ = Subset the design by rows. 

e.g.({\ttfamily design\$subset\_rows(1, 100)} )

\end{itemize}

%----------------------------------------------------------------------------------------
\newpage

\section{Population Total} \label{c2:tot} \label{c2.2}
%http://essedunet.nsd.uib.no/cms/topics/weight/3/1.html



The function {\ttfamily svydbtotal()} was designed to estimate the population total in {\sf R} by using {\bf dplyr}, it is compatible with data frames and sql tables and was designed to do as much computation as possible in a database.\\

In the function {\ttfamily svydbtotal()}, the total is computed by using the Horvitz-Thompson estimator \citep{hte}, it is an unbiased estimator of the population total.

$$ \hat{Total} = \sum_{h = 1}^{L} \sum_{i = 1}^{m_h} z_{hi}$$

where,

$$z_{hi} = \sum_{j \in PSU} w_{hij} x_{hij}$$

\begin{itemize}
\item $L$ = number of stratum

\item $m_h$ = number of clusters in stratum $h$

\item $w_{hij}$ = the sample weight in stratum {\emph h} and cluster {\emph i} for observation {\emph j}.
\end{itemize}
\\
Variance estimation of the total uses the Horvitz-Thompson estimator on an influence function.

$$Var(\hat{Total}) =  \sum_{h = 1}^{L} \frac{m_h}{m_h - 1} 
                    \sum_{i = 1}^{m_h} (z_{hi} - \bar{z}_h)^T (z_{hi} - \bar{z}_h)$$

where,
$$\bar{z}_h = \frac{1}{m_h} \sum_{i = 1}^{m_h} z_{hi}$$
\\
\subsection{Usage}
\begin{center}
    {\ttfamily svydbtotal(x, num = T, return.total = F, design)}
\end{center}
\subsection{Arguments}
\begin{itemize}
\item $x$ = Name indicating the variable.

\item $num$ = {\ttfamily TRUE} or {\ttfamily FALSE} indicating whether x is numeric or categorical.

\item $return.total$ = {\ttfamily TRUE} to return only totals, no standard errors.

\item $design$ = svydb.design object.
\end{itemize}

\newpage
\subsection{Examples}
\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                id = SDMVPSU, data = nhdb)
> svydbtotal(x = Race3, design = nh.dbsurv, num = T)
#           Total       SE
# Race3 959380842 61432595

> svydbtotal(x = Race3, design = nh.dbsurv, num = F)
#             Total       SE
# Race3_1  29812316  6112527
# Race3_2  21416164  4485865
# Race3_3 192721267 23431296
# Race3_4  38131538  5561161
# Race3_6  15519529  2367723
# Race3_7   8989867  1468813

> svydbtotal(x = Race3, design = nh.dbsurv , num = T, 
    return.total = T)
#           Total
# Race3 959380842
\end{lstlisting}
\\

Generic functions like {\ttfamily coef()} and {\ttfamily SE()} were also implemented to extract the coefficients and standard errors from a {\bf svydbstat} object.
\begin{lstlisting}
> class(svydbtotal(x = Race3, design = nh.dbsurv, num = T))
# [1] "svydbstat"

> coef(svydbtotal(x = Race3, design = nh.dbsurv , num = T))
# [1] 959380842

> SE(svydbtotal(x = Race3, design = nh.dbsurv , num = T))
#    Race3 
# 61432595 
\end{lstlisting}
\\
\subsection{Difficulties}
\begin{enumerate}
\item In {\sf SQL} if a column contains only numbers, it is not possible to identify whether a column type is factor or numeric, therefore in the {\ttfamily svydbtotal()} function, the user needs to specify it. {\ttfamily num = TRUE}: Numeric, {\ttfamily num = FALSE}: Factor. \label{tot:d1}

\item To compute the population total for a categorical variable, dummy variables are needed. In {\sf SQL}, there are two ways to do this. The first way would be to create new columns for every levels of the variable manually and apply {\ttfamily ifelse/CASE WHEN} to each of the columns. Another way would be to create a small contrast table in memory and use {\ttfamily INNER JOIN} to join the small table onto the data set. The second approach is much faster, and the function {\ttfamily dummy\_mut()} adapts that approach. It creates (mutate) new dummy columns on the right side of the data set. \label{tot:d2}

\item Calculating the variance/standard error is the most complex part of {\ttfamily svydbtotal()}. Therefore the {\ttfamily svyVar()} function is written to calculate the variance of a variable. If the variable is a categorical variable with multiple levels, the calculations will be replicated with {\ttfamily sapply()}. \label{tot:d3}
\end{enumerate}

%----------------------------------------------------------------------------------------
\newpage
\section{Population Mean}
The function {\ttfamily svydbmean()} was designed to estimate the population mean in {\sf R} by using {\bf dplyr}, it is compatible with data frames and sql tables and was designed to do as much computation as possible in a database.\\

In the function {\ttfamily svydbmean()}, the mean is computed by using a ratio estimator rather than the  Horvitz-Thompson estimator. This is a standard in survey software as $N$ may not be known.

$$ \hat{Mean} = \frac{1}{\hat{N}} \sum_{h = 1}^{L} \sum_{i = 1}^{m_h} z_{hi}$$

where,

$$\hat{N} = \sum_{j \in PSU} w_j \text{,\quad} z_{hi} = \sum_{j \in PSU} w_{hij} x_{hij}$$
\\
\begin{itemize}
\item $L$ = number of stratum

\item $m_h$ = number of clusters in stratum $h$

\item $w_{hij}$ = the sample weight in stratum {\emph h} and cluster {\emph i} for observation {\emph j}.
\end{itemize}
\\
Variance estimation of the mean uses the Horvitz-Thompson estimator on an influence function.

$$Var(\hat{Mean}) =  \sum_{h = 1}^{L} \frac{m_h}{m_h - 1} 
                    \sum_{i = 1}^{m_h} (d_{hi} - \bar{d}_h)^T (d_{hi} - \bar{d}_h)$$

where,
$$d_{hi} = \frac{1}{\hat{N}} \sum_{j \in PSU} w_{hij}(x_{hij} - \bar{x}) 
\text{,\quad} 
\bar{d}_h = \frac{1}{m_h} \sum_{i = 1}^{m_h} d_{hi}$$
\\

\subsection{Usage}
\begin{center}
    {\ttfamily svydbmean(x, num = T, return.mean = F, design)}
\end{center}
\subsection{Arguments}
\begin{itemize}
\item $x$ = Name indicating the variable.

\item $num$ = {\ttfamily TRUE} or {\ttfamily FALSE} indicating whether x is numeric or categorical.

\item $return.mean$ = {\ttfamily TRUE} to return only means, no standard errors.

\item $design$ = svydb.design object.
\end{itemize}

\newpage
\subsection{Examples}
\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                id = SDMVPSU, data = nhdb)
> svydbmean(x = Race3, design = nh.dbsurv , num = T)
#         Mean     SE
# Race3 3.1292 0.0674

> svydbmean(x = Race3, design = nh.dbsurv, num = F)
#             Mean     SE
# Race3_1 0.097238 0.0208
# Race3_2 0.069853 0.0154
# Race3_3 0.628595 0.0407
# Race3_4 0.124373 0.0239
# Race3_6 0.050620 0.0080
# Race3_7 0.029322 0.0045
\end{lstlisting}
\\

Generic functions like {\ttfamily coef()} and {\ttfamily SE()} were also implemented to extract the coefficients and standard errors from a {\bf svydbstat} object. This is useful since the {\bf svydbstat} objects are rounded when they are printed, by using {\ttfamily coef()} and {\ttfamily SE()}, the unrounded value can be extracted.
\begin{lstlisting}
> class(svydbmean(x = Race3, design = nh.dbsurv, num = T))
# [1] "svydbstat"

> coef(svydbmean(x = Race3, design = nh.dbsurv, num = T))
# [1] 3.129191

> SE(svydbmean(x = Race3, design = nh.dbsurv, num = T))
#      Race3 
# 0.06735437
\end{lstlisting}
\\
\subsection{Difficulties}
\begin{enumerate}
\item In {\sf SQL} cannot recognise factor variables. Explained in difficulty \ref{tot:d1} from chapter \ref{c2.2} (\hyperref[c2:tot]{Population Total}).

\item {\sf SQL} does not have built-in support for dummy variables. Explained in difficulty \ref{tot:d2} from chapter \ref{c2.2} (\hyperref[c2:tot]{Population Total}).

\item Difficult to compute the variances. Explained in difficulty \ref{tot:d3} from chapter \ref{c2.2} (\hyperref[c2:tot]{Population Total}).
\end{enumerate}

%----------------------------------------------------------------------------------------

\newpage
\section{Regression}
The function {\ttfamily svydblm()} was designed to fit a linear model to survey data in {\sf R} by using {\bf dplyr}, it is compatible with data frames and sql tables and was designed to do as much computation as possible in a database.\\

In the function {\ttfamily svydblm()}, the coefficients are computed by,
$$\hat{\beta} = (X^TWX)^{-1}(X^TWY)$$
\begin{itemize}
\item $W$ = Sampling weights
\end{itemize}
\\
Variance estimation of the coefficients uses a similar approach as survey mean/total,
$$Var_{pq}(\hat{\hat{\beta}}) =  \sum_{h = 1}^{L} \frac{m_h}{m_h - 1} 
                    \sum_{i = 1}^{m_h} (z_{hip} - \bar{z}_{hp})^T (z_{hiq} - \bar{z}_{hq})$$
where,
$$z_{hi} = x_{hij} w_{hij} (y_{hij} - \mu_{hij}) 
\text{,\quad} 
\bar{z}_h = \frac{1}{m_h} \sum_{i = 1}^{m_h} z_{hi}$$
\\
And by using the sanwich estimator,
$$cov(\hat{\beta}) =  (X^TWX)^{-1}Var_{pq}(\hat{\beta})(X^TWX)^{-1}$$

\begin{itemize}
\item $L$ = number of stratum

\item $m_h$ = number of clusters in stratum $h$

\item $w_{hij}$ = the sample weight in stratum {\emph h} and cluster {\emph i} for observation {\emph j}.

\item $p,q$ = Indicator function for variables $p$ and $q$.
\end{itemize}

\subsection{Usage}
\begin{center}
    {\ttfamily svydblm(formula, design)}
\end{center}
\subsection{Arguments}
\begin{itemize}
\item $formula$ = Model formula.

\item $design$ = svydb.design object.
\end{itemize}

\newpage
\subsection{Examples}
\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                        id = SDMVPSU, data = nhdb)
> svydblm(DirectChol ~ Age + BMI + factor(Gender), 
    design = nh.dbsurv)
# svydb.design, 9756 observation(s), 31 Clusters
#
# Survey design:
# svydbdesign(st = SDMVSTRA, id = SDMVPSU, wt = WTMEC2YR, 
#    data = nhdb)
#
# Call:
# svydblm(formula = DirectChol ~ Age + BMI + factor(Gender), 
#    design = nh.dbsurv)
#
# Coefficients:
#       intercept  Age        BMI        Gender_2 
# [1,]   1.632111   0.003254  -0.018636   0.218086
\end{lstlisting}

\\
The {\ttfamily summary()} function to obtain the summary of the model and {\ttfamily predict()} function to predict using new data-sets with the model was also implemented,
\\
\begin{lstlisting}
# dbfit = svydblm(formula = DirectChol ~ Age + BMI, 
#   design = nh.dbsurv)

> summary(dbfit)
# Call:
# svydblm(formula = DirectChol ~ Age + BMI,
#   design = nh.dbsurv)
#
# Survey design:
# svydbdesign(st = SDMVSTRA, id = SDMVPSU, wt = WTMEC2YR, 
#   data = nhdb)
#
# Coefficients:
#             Estimate Std. Error t value Pr(>|t|)    
# intercept  1.7263135  0.0279703  61.719  < 2e-16 ***
# Age        0.0034161  0.0003428   9.966 5.23e-08 ***
# BMI       -0.0182468  0.0011277 -16.181 6.63e-11 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 
#   0.05 ‘.’ 0.1 ‘ ’ 1

> predict(dbfit, newdata = data.frame(Age = 1:3, BMI = 4:6))
#     link     SE
# 1 1.6567 0.0242
# 2 1.6419 0.0233
# 3 1.6271 0.0225
\end{lstlisting}
\\\newpage
Other generic function includes {\ttfamily coef()}, {\ttfamily SE()}, {\ttfamily vcov()} and {\ttfamily residuals()}.
\\
\begin{lstlisting}
# dbfit = svydblm(formula = DirectChol ~ Age + BMI, 
#   design = nh.dbsurv)

> coef(dbfit)
#      intercept         Age         BMI
# [1,]  1.726314 0.003416124 -0.01824676

> SE(dbfit)
#    intercept          Age          BMI 
# 0.0279703171 0.0003427746 0.0011276877

> vcov(dbfit)
#               intercept           Age           BMI
# intercept  7.823386e-04  3.769663e-06 -2.806340e-05
# Age        3.769663e-06  1.174944e-07 -2.496403e-07
# BMI       -2.806340e-05 -2.496403e-07  1.271680e-06

> head(residuals(dbfit), 3) 
# Source:   lazy query [?? x 1]
# Database: MonetDBEmbeddedConnection
#   residuals
#       <dbl>
# 1    -0.316
# 2    -0.318
# 3    -0.733
\end{lstlisting}



\\
\subsection{Difficulties}
\begin{enumerate}
\item {\sf SQL} does not have built-in support for dummy variables. Explained in difficulty \ref{tot:d2} from chapter \ref{c2.2} (\hyperref[c2:tot]{Population Total}).

\item In {\sf SQL} matrix multiplications are not supported, however, it can still be implemented since matrix multiplications only requires addition and multiplication. For example with matrix $X$, by calculating the sums of products of the first column of the matrix with the rest of the columns (including the first column), we get the first row of the $X^TX$ matrix. To get the whole $X^TX$ matrix we only need to repeat the process with different columns. However, the inverse of a matrix is not possible in {\sf SQL}, so to compute $(X^TX)^{-1}$, we need to pull the matrix into memory. 

\item The variance for the regression coefficients are a bit more complicated than computing the variance for mean/total, since for mean/total we only need the diagonal of the co-variance matrix. However, the variances of the regression coefficients requires the whole covariance matrix. This means that more replication with different combinations of $z_{hip}$/$z_{hiq}$ will be needed, but we only need the upper triangle or the lower triangle of the matrix, since it is a symmetric matrix.
\end{enumerate}



%----------------------------------------------------------------------------------------
\newpage
\section{Quantiles}

The function {\ttfamily svydbquantile()} was designed to compute the medians/quantiles from survey data in {\sf R} by using {\bf dplyr}, it is compatible with data frames and a {\bf few types} of sql tables and was designed to do as much computation as possible in a database.\\

To estimate the median/quantiles, a standard probabilistic algorithm is used. For example to estimate the median,
\begin{enumerate}
    \item \label{qe1} Take a sample of size $n^{2/3}$ from the data set and read it into memory. (Proof below)

    \item \label{qe2} Compute the 99\% confidence interval $[a,b]$ of the 0.5 quantile by using {\ttfamily svyquntile()} from the survey package.
    
    \item \label{qe3} Read in the data set where the observations are between $a$ and $b$.
    
    \item \label{qe4} Sort the data and compute the cumulative sum of the weights of the read in observations, $w_{n} = \sum_{i = 1}^{n} w_i$
    
    \item \label{qe5} Find out if the median is within the read in data set. Since $median = 0.5 \times W$, we can find out by searching if median equals or is between $w_{n}$.
    
    \item \label{qe6} If the median is not found, repeat.
\end{enumerate}
\\
Other quantiles are computed with the same method. \\

        
Though by using this method to compute the survey quantile requires at least two sets of data to be read into memory, however it is still much more efficient than sorting and calculating the cumulative sum for the whole data set, since $n^{2/3}$ is relatively small compared to whole data set (One million observations, $10000000^{2/3} = 10000$).

\hspace{1}

\begin{tcolorbox}
%\begin{proof}
   Let $M$ be a random sample of $N$.}\\
   
   The first set of data read into memory has $M$ points and its confidence interval length is $\propto M^{-1/2}$. (step \ref{qe1}) \\
   
   Number of points in the second read is $\beta N M^{-1/2}$. (step \ref{qe3}) \\
   
   Total number of points read in is $M + \beta N M^{-1/2}$. Minimise,
   
   $$\frac{\partial}{\partial M} = 1 + N \left(\frac{-1}{2} M^{-3/2} \right) = 0$$
   
   $$1 = \frac{1}{2} N M^{-3/2}$$
   
   $$2M^{-3/2} = N$$
   
   $$M \propto N^{2/3}$$
%\end{proof}
\end{tcolorbox}



\newpage

\subsection{Usage}
\begin{center}
    {\ttfamily svydbquantile(x, quantiles = 0.5, design)}
\end{center}
\subsection{Arguments}
\begin{itemize}
\item $x$ = Name indicating the variable.

\item $quantiles$ = Quantiles to estimate, a number, or a vector of numbers for multiple quantiles. Default to 0.5.

\item $design$ = svydb.design object.
\end{itemize}

\subsection{Examples}
\begin{lstlisting}
> db.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                        id = SDMVPSU, data = nhdb)
> svydbquantile(x = Age, quantile = 0.5, design = nh.dbsurv)
# 0.5 
#  37 
 
> svydbquantile(x = BMI, quantile = c(0.25,0.75), 
                            design = nh.dbsurv)
# 0.25 0.75 
# 21.7 30.6 
\end{lstlisting}
\\
\subsection{Difficulties}
\begin{enumerate}
\item To compute the survey quantile, a sample of the data-set is needed. However, different types of {\sf SQL} uses different queries for sampling tables and the {\ttfamily sample\_frac()} function from {\bf dplyr} currently only works with {\bf Spark} \citep{sparkpackage} database connections and local dataframes/tibbles. Sampling in {\bf MonetDB} \citep{monetdb} was also implemented. Therefore currently, the {\ttfamily svydbquantile()} is only tested with local dataframes and database connections that are mentioned above. It may be possible that it is compatible with other types of connections but they are to be tested. Another possibility is that the {\ttfamily sample\_frac()} function will be extended in the future to support more database connections.

\item Since to compute the survey quantile in {\ttfamily svydbquantile()}, the inputted data-set will be sized down into $n^{2/3}$, so if the inputted data-set is a pre-subsetted data-set then it means that the data-set will be subsetted at least twice to compute the quantiles. This could be a problem because {\ttfamily svydbquantile()} runs through {\ttfamily svyquantile()} form the {\bf survey} package which uses {\ttfamily svymean()} within it. If the data-set is too small it may lose enough information and may cause mathematical errors. For example, if there are only one cluster within a strata then it will cause the equation to divide by zero. 
\\
To overcome this, there is a option within the {\bf survey} package called "survey.loney.psu", if we set this option to "adjust", e.g. {\ttfamily options("survey.lonely.psu" = "adjust")}, it will allow survey statistic computations even if there is only one cluster within a strata.

\end{enumerate}

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\newpage
\section{Survey Tables}
The function {\ttfamily svydbtable()} was designed to create contingency tables for survey data, it is compatible with data frames and sql tables and was designed to do as much computation as possible in a database.\\

Each cell within the table is computed with the same method as {\ttfamily svydbtotal()}.

\subsection{Usage}
\begin{center}
    {\ttfamily svydbtable(formula, design, as.local = F)}
\end{center}
\subsection{Arguments}
\begin{itemize}
\item $formula$ = A formula specifying margins for the table, only + can be used.

\item $design$ = svydb.design object.

\item $as.local$ = A logical value indicating the returning object type. Default is database tables, {\ttfamily tbl\_sql}.
\end{itemize}

\subsection{Examples}

\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                        id = SDMVPSU, data = nhdb)

> svydbtable(~MaritalStatus, design = nh.dbsurv)
# Source:     lazy query [?? x 2]
# Database:   MonetDBEmbeddedConnection
# Ordered by: MaritalStatus
#   MaritalStatus         wt
#           <int>      <dbl>
# 1             1 118752657.
# 2             2  12600347.
# 3             3  23868539.
# 4             4   5486968.
# 5             5  44543092.
# 6             6  18664186.                       

> svydbtable(~MaritalStatus, design = nh.dbsurv, 
    as.local = T)
#   MaritalStatus         wt
#           <int>      <dbl>
# 1             1 118752657.
# 2             2  12600347.
# 3             3  23868539.
# 4             4   5486968.
# 5             5  44543092.
# 6             6  18664186.
\end{lstlisting}
\newpage
\begin{lstlisting}
> svydbtable(~Race3 + Smoke100, design = nh.dbsurv, 
    as.local = T)
# A tibble: 6 x 5
#   Race3 Smoke100_1 Smoke100_2 Smoke100_7 Smoke100_9
#   <int>      <dbl>      <dbl>      <dbl>      <dbl>
# 1     1   6177437.  11124548.         0          0 
# 2     2   5375656.   9313414.         0          0 
# 3     3  71234238.  77533263.         0      29247.
# 4     4   9687657.  16031251.     22142.     14686.
# 5     6   3019921.   8645276.         0      14966.
# 6     7   3070772.   2650765.         0          0 


> svydbtable(~Race3 + Work + Gender, design = nh.dbsurv, 
    as.local = T)

# $`Gender  =  1`
# A tibble: 6 x 6
#   Race3    Work_1   Work_2   Work_3    Work_4 Work_7
#   <int>     <dbl>    <dbl>    <dbl>     <dbl>  <dbl>
# 1     1  7063412.  118589.  805525.  2148047. 54555.
# 2     2  5066507.   50966.  283436.  2056117.     0 
# 3     3 48916000. 1670881. 3052152. 22756262.     0 
# 4     4  6557730.  166729.  812680.  5201428.     0 
# 5     6  3689443.   79793.  191406.  1764683.     0 
# 6     7  1747070.   34664.  258597.  1182825.     0 

# $`Gender  =  2`
# A tibble: 6 x 6
#   Race3    Work_1   Work_2   Work_3    Work_4 Work_7
#   <int>     <dbl>    <dbl>    <dbl>     <dbl>  <dbl>
# 1     1  4839422.   80842.  273991.  4040679.      0
# 2     2  4052770.   20057.  400463.  3907392.      0
# 3     3 42775016. 1370755. 2995602. 34142499.      0
# 4     4  7486159.  280862.  915095.  7070385.      0
# 5     6  3557006.  111248.  209497.  2803386.      0
# 6     7  1810965.   17984.  114653.  1255186.      0

\end{lstlisting}
\\

%----------------------------------------------------------------------------------------
\newpage
\section{Survey Statistic on Subsets}
The function {\ttfamily svydbby()} was designed to compute survey statistics on subsets of the data in {\sf R} by using {\bf dplyr}, it is compatible with data frames and sql tables and was designed to do as much computation as possible in a database.\\

This function creates a number of subsets based on the conditions given by the user and computes the desired survey statistic on all the subsets. Currently, it is only compatible with {\ttfamily svydbtotal()} and {\ttfamily svydbmean()}.\\

\subsection{Usage}
\begin{center}
    {\ttfamily svydbby(x, by, FUN, design, ...)}
\end{center}

\subsection{Arguments}
\begin{itemize}
\item $x$ = A variable specifying the variable to pass to FUN.

\item $by$ = A variable specifying factors that define the subsets.

\item $FUN$ = A function indicating the desired survey statistics.

\item $design$ = svydb.design object.

\item $...$ = Other arguments to pass to FUN.
\end{itemize}

\subsection{Examples}
\begin{lstlisting}
> nh.dbsurv = svydbdesign(st = SDMVSTRA, wt = WTMEC2YR, 
                        id = SDMVPSU, data = nhdb)
> svydbby(x = Age, by = Gender, FUN = svydbmean, 
                design = nh.dbsurv, num = T)
# $Age
#                 Mean        SE
# Gender == 1 36.21035 0.8387459
# Gender == 2 38.09899 0.6721789

> svydbby(x = BMI, by = Race3, FUN = svydbtotal, 
                design = nh.dbsurv, num = T)
# $BMI
#                 Total        SE
# Race3 == 3 5004822100 612188966
# Race3 == 1  737939057 153551662
# Race3 == 6  347435902  52876464
# Race3 == 4 1020476221 154630793
# Race3 == 7  227729724  39637006
# Race3 == 2  550207484 118331941
\end{lstlisting}
\\

%----------------------------------------------------------------------------------------

\newpage
\section{Replicate Weights}
In survey data sets, standard errors can never be known with any certainty and are only estimated. Replicate weights lets us to use a single sample with different sampling weights to capture the characteristics of multiple samples, it allows us to compute more informed standard error estimates, this method is similar to bootstrap and jackknife sampling. Though computing standard errors from replicate weights usually result in them getting bigger, but the increase usually is not large enough that it can alter the significance level. 

Another reason for us to use replicate weights is that it provides a less complex way to compute the standard errors. \\

Currently, replicate weights are available in a number of data sets, for example the American Community Survey and Puerto Rican Community Survey data. In these data sets, there are 80 separate replicate weights at the household/person level which allows us to compute the standard errors.

\subsection{Replicate Standard Errors}
To compute the replicate standard errors, there are three steps.
\begin{enumerate}
    \item Compute the survey statistics of interest with the full sample weights.
    
    \item Rerun the analysis with each set of the replicate weights.
    
    \item Calculate the standard error,
    
            $$ SE(X) = \sqrt{ s \sum_{r = 1}^{n(r)} (X - X_r)^2 }$$
                
            \begin{itemize}
                \item $X$ = Result of the survey statistics using the full sample weights.

                \item $X_r$ = Result of the survey statistics using the r'th set of the replicate weights. 

                \item $s$ = Scale multiplier. i.e. $\frac{4}{80}$ for the American Community Survey.
            \end{itemize}

\end{enumerate}

\subsection{Survey Replicate Design}

Similarly, there is a survey replicate design like the survey design from section \ref{c2.1}.\\

Currently, replicate statistics that are supported are survey totals and means. Therefore, there is no need to provide the stratification and clustering information to {\ttfamily svydbrepdesign()}.

\begin{center}
    {\ttfamily svydbrepdesign(wt, repwt, scale, data)}
\end{center}

\begin{itemize}
\item $wt$ = Column name specifying the sampling weights column.

\item $repwt$ = A regular expression that matches the names of the replication weight variables.

\item $data$ = A data frame or sql table of the survey data set.
\end{itemize}

\subsection{Examples}
\begin{lstlisting}
> hde.repsurv = svydbrepdesign(wt = WGTP, repwt="wgtp[0-9]+", 
      scale = 4/80, data = ss16hde)
> hde.repsurv
# svydb.repdesign, 4582 observation(s), 
#   80 sets of replicate weights, scale = 0.05
\end{lstlisting}

\subsection{Population Total with replicate weights}

Arguments are the same as {\ttfamily svydbtotal()}, but with an extra argument. \\
\begin{itemize}
    \item $return.replicate$ = $TRUE$ to return the replicate statistics.
\end{itemize}

\begin{lstlisting}
> hde.dbrepsurv = svydbrepdesign(wt = WGTP, 
    repwt = "wgtp[0-9]+", scale = 4/80, data = ss16hde)
> svydbreptotal(x = BATH, design = hde.dbrepsurv , num = T)
#       Total     SE
# BATH 429410 755.04

> svydbreptotal(x = FS, design = hde.dbrepsurv , num = F)
#       Total     SE
# FS_1  37392 2151.8
# FS_2 313694 3033.6

> (svydbreptotal(x = HHT, design = hde.dbrepsurv , 
        num = T, return.replicates = T)$replicates)[,1:3]
#    wgtp1  wgtp2  wgtp3
#    <dbl>  <dbl>  <dbl>
# 1 975397 973284 987185

> coef(svydbreptotal(x = BATH, design = hde.dbrepsurv,
        num = T))
# [1] 429410      

> SE(svydbreptotal(x = BATH, design = hde.dbrepsurv, 
    num = T))
#     BATH  
# 755.0406    
\end{lstlisting}
\subsection{Population Mean with replicate weights}
All arguments are the same as {\ttfamily svydbreptotal()}.
\begin{lstlisting}
> svydbrepmean(x = BATH, design = hde.dbrepsurv , num = T)
#        Mean     SE
# BATH 1.0076 0.0018

> svydbrepmean(x = FS, design = hde.dbrepsurv , num = F)
#        Mean     SE
# FS_1 0.1065 0.0061
# FS_2 0.8935 0.0061
\end{lstlisting}