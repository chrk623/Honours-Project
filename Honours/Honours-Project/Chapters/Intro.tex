% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Introduction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Background}\label{c1.1}

Currently, there is already a {\bf survey} package  \citep{surveypackage} in {\sf R} which is at a stable production status, it provides survey analysis, including graphics, estimation and inference. It also supports both replicate-weight and Taylor linearisation standard errors, and can efficiently handle multistage stratified designs without replacements. However, it requires the data sets to be stored in a data frame in memory. For most survey data sets this is not a problem, however, nowadays there are a number of large survey data sets, for example the American Community Survey (ACS) includes 3,000,000 people per year, and the Nationwide Emergency Department Sample (NEDS) includes more than 25,000,000 hospital visit records per year. \\

In {\sf R}, there are currently two approaches to analyse survey data sets in a database. The first is to use the {\bf survey} package, with its database back-end function, the data sets can be loaded into memory without any problem, but the time to analysis the data may not be promising when the data sets are too large.\\

Another approach is to perform as much computation as possible directly in the database, so that only small bits of data or numbers are transferred into memory when necessary. This approach is more efficient but is less flexible, since mathematics and statistical operations are limited in a database. Another advantage of this approach is that if the database is powerful, then the computation would be faster than just using a standard laptop or desktop. 

The second approach is implemented in the {\bf sqlsurvey} {\sf R} package \citep{sqlsurveypackage}}, however, codes which communicates with the databases are written in "hand-written {\sf SQL} code". Therefore, it would be hard to maintain and would cause compatibility issues between different types of databases. Not only codes may look different, there is also a major inconsistency in evaluating the code, for example dealing with missing values. Despite the attempt of standardising the {\sf SQL} standards between multiple companies, issues of portability still remains.\\

The better approach to analyse survey data sets in a database would be to use the {\sf R} packages {\bf dplyr} \citep{dplyrpackage} and {\bf dbplyr} \citep{dbplyrpackage} as a database interface. Since these packages are maintained by experts at Rstudio, it is likely that these packages are more stable than others, bug fixes and updates would also be quick. Most importantly, it is more portable where its compatibility extends to powerful databases back ends like PostgreSQL and Google BigQuery. So, this project will implement a set of functions to analyse survey data sets with the second approach, named {\bf svydb}, and evaluate its speed on large survey data sets.

\section{Survey data in SQL}
As mentioned in section \ref{c1.1}, when we are analysing large survey data sets, it would be more feasible to do it in a database. Some commonly used survey statistics are survey mean, survey total, summaries and regression.\\

Survey totals, means and summaries can be easily computed in a database, since it only requires simple arithmetic like summing, multiplications, divisions along with some grouping. 

Regression may require a bit more work, since it requires matrix operations which are not supported in {\sf SQL}, however by loading a few chunks of small matrices into memory, it can still be easily implemented in a database, since after all regression coefficients and their variances only requires sums and multiplications.\\

More details of the calculations and difficulties will be discussed later on in Chapter \ref{c2}.

\section{Coding with {\bf dplyr} and {\bf dbplyr}}

\subsection{Introduction to dplyr} \label{c1.2.1}
The {\bf dplyr} package was implemented to manipulate, clean and construct data. With this package, data manipulation and data exploration can be done easily and quickly, since they are written in a computationally efficient manner. 

The package contains a few common data manipulating functions such as selecting specific columns, arranging or creating new columns, filtering rows, merging data (joins) and summarising data by groups. Other features such as simple statistics operations are also included in the package.

\subsection{Pipes}
The pipe operator ({\ttfamily \%>\%}) first appeared in {\bf magrittr} package \citep{magrittrpackage}, and is created to make codes more readable.
The pipe operator inputs the object on the left-hand side of the pipe into the function on right-hand side. Some basic piping are as follows:

\begin{itemize}

\item {\ttfamily x \%>\% f} is equivalent to {\ttfamily f(x).}

\item {\ttfamily x \%>\% f(y)} is equivalent to {\ttfamily f(x, y).} 

\end{itemize}
\\
\vspace{10}
It is rather useful when we have multiple steps while we are transforming data sets, because naturally we read from left to right. For example, with traditional coding, reading is always from inside out,
\begin{lstlisting}
> x = sample(10)
> summary(diff(exp(floor(cos(x)))))
\end{lstlisting}
with piping, it is much easier to read,
\begin{lstlisting}
> x %>% cos() %>% floor() %>% exp() %>% diff() 
    %>% summary() 
\end{lstlisting}
\\
\vspace{10}
Though the pipe has its advantages, there are also times that it is not useful. It would not be useful when the intermediate variables are needed or when the intermediate variables require heavy computation.

\subsection{dplyr's SQL compatibility (dbplyr)}
As mentioned in section \ref{c1.2.1}, there are six basic functions in {\bf dplyr}. These functions are all related to the basic {\sf SQL} queries.

%https://www.listendata.com/2016/08/dplyr-tutorial.html
\begin{center}
\begin{tabular}{ |l|l|l|l| } 
\hline
\textbf{dplyr Function} & \textbf{Description} & \textbf{Equivalent SQL} \\
\hline
select() & Selecting columns (variables) & SELECT \\ 
filter () & Filter (subset) rows. & WHERE \\ 
group\_by() & Group the data & GROUP BY \\ 
arrange() & Sort the data & ORDER BY \\ 
join() & Joining tables & JOIN \\ 
mutate() & Creating New Variables (Columns) & COLUMN ALIAS \\ 
\hline
\end{tabular}
\end{center}

With {\bf dbplyr}, when these {\bf dplyr} functions are applied onto a sql table, they automatically translate itself into {\sf SQL} queries. For example,

\begin{lstlisting}
> mtdb %>% select(mpg, gear) %>% group_by(gear) %>% 
    summarise(sum_mpg = sum(mpg)) %>% head(3) %>% 
    show_query()
 
 # <SQL>
 # SELECT "gear", SUM("mpg") AS "sum_mpg"
 # FROM (SELECT "mpg" AS "mpg", "gear" AS "gear"
 # FROM "mtcars") "gaecowztcc"
 # GROUP BY "gear"
 # LIMIT 3
\end{lstlisting}
With this approach, {\bf dplyr} does not actually do any work, its job is only to translate the codes into {\sf SQL} and gives the database instructions. Another advantage of this method is that the intermediate variables between the pipes only builds up the query and does not get evaluated nor is stored anywhere. 

\subsection{Quasi-quotation}
Programming with {\bf dplyr} relies on a concept called the quasi-quotation, also known as non-standard evaluation, it means that while we are doing some evaluation with {\bf dplyr} in {\sf R}, we are not using {\sf R}'s standard evaluation method. For example, with {\sf R}'s standard method, 

\begin{lstlisting}
test_func = function(x, y){
    x + y
}  
> x1 = 1; x2 = 2
> test_func(x1, x2)
\end{lstlisting}
{\sf R} looks for the variables {\ttfamily x1} and {\ttfamily x2} in the environment, evaluates them and input their values into the function {\ttfamily test\_func}. \\

However, while programming in {\bf dplyr},
\begin{lstlisting}
> mtcars %>% select(mpg)
\end{lstlisting}
The variable {\ttfamily mpg} is a variable in the data set and cannot be found in the environment, it is quoted and evaluated in a non standard way.\\

Though, it might look useful to use this non-standard evaluation method, but it is more difficult to program with, for example while writing a function,

\begin{lstlisting}
test_fun2 = function(data, x){
    data %>% select(x) %>% head(2)
}
> test_fun2(mtcars, mpg)
# Error: `x` must resolve to integer column positions, 
# not a list 
\end{lstlisting}

Since {\bf dplyr} does not evaluate in the standard way, we cannot just pass a variable in like the standard method. We will need to quote it with the {\ttfamily quo()} or {\ttfamily enquo()} function.

\begin{lstlisting}
test_fun2 = function(data, x){
    x = enquo(x)
    data %>% select(!!x) %>% head(2) %>% tbl_df()
}
> test_fun2(mtcars, mpg)
    
#    mpg
#* <dbl>
#1    21
#2    21
\end{lstlisting}
{\ttfamily enquo()} allows us to quote the variable resulting in a quosure where it contains its expression along with an evaluation environment, so we can pass it into the {\ttfamily select()} function, and {\ttfamily !!} (bang bang) allows us to unquote the variable at evaluation. \\

There are also other similar functions to help us overcome the difficulties while programming with  {\bf dplyr}, like {\ttfamily quos()}, {\ttfamily sym()} and {\ttfamily quo\_name()} which are used in different situations.

\subsection{Common issues while coding with dplyr in a database}

\begin{itemize}
    \item No factor types in {\sf SQL}.
    \item Difficult to code with quasi-quotation.
    \item Cannot do row-wise operations due to the lazy interface. That is, the data sets within a database in {\sf R} will not be loaded into memory unless required.
    \item No matrix operations.
    \item No base {\sf R} functions. 
    \item No distributions.
    \item Inconsistent availability of functions between databases.
\end{itemize}

%\newpage

\section{Layout}

In chapter \ref{c2}, estimation methods and functions will be discussed, and in chapter \ref{c3}, graphics. 

In chapter \ref{c4}, speed of database-based and memory-based implementations will be compared,  chapter \ref{c5} discuss the usability of the functions, and lastly, a discussion in chapter \ref{c6}.